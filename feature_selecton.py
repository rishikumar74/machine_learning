# -*- coding: utf-8 -*-
"""feature_selecton.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YBAaj9zbCyns2fX-_IpzDWCPHkCCbjU4

# variance threshold
"""

import pandas as pd
import numpy as np
data=pd.DataFrame({
    'age':[25,30,35,40,45],
    'salary':[5000,6000,7000,8000,9000],
    'country':[1,1,1,1,1]
})

data

from sklearn.feature_selection import VarianceThreshold
selector=VarianceThreshold(threshold=0.1)
selected_data=selector.fit_transform(data)

selected_features=data.columns[selector.get_support()]
print("selected features",selected_features)

"""# **correlational coeff**"""

import numpy as np
import pandas as pd
data=pd.DataFrame({
    'sqft':[1000,1500,2000,2500,3000],
    'tiles_cnt':[100,150,200,250,300],
    'house_price':[200000,300000,400000,500000,600000]
})

data

correlation_matrix=data.corr()
correlation_matrix

import matplotlib.pyplot as plt
import seaborn as sns
sns.heatmap(correlation_matrix,annot=True,cmap='RdBu_r')
plt.show()

#remove highly correlated coeff(>0.9)

upper=correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape),k=1).astype(bool)
)
to_drop=[column for column in upper.columns if any(upper[column]>0.9)]
print("highly correlated features to drop: ",to_drop)



"""# chi-square **test**"""

import pandas as pd
x=pd.DataFrame({
    'gender':[0,1,0,1,1],
    'married':[1,1,0,0,1],
    'income':[20000,40000,50000,30000,60000]
})
y=[0,1,1,0,1]
x

#chi square requires non negetive values
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest
from sklearn.preprocessing import MinMaxScaler


scaler=MinMaxScaler()
x_scaled=scaler.fit_transform(x)

selector=SelectKBest(score_func=chi2,k=2)
x_new=selector.fit_transform(x_scaled,y)
selected_features=x.columns[selector.get_support()]
print("Selected features: ", selected_features)

"""# wrapper **methods**"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
df=load_breast_cancer()
x=pd.DataFrame(df.data,columns=df.feature_names)
x.head()

x.info()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SequentialFeatureSelector

y=df.target
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
model=LogisticRegression(max_iter=500)

#forward Selection
sfs=SequentialFeatureSelector(model,n_features_to_select=5,direction='forward')
sfs.fit(x_train,y_train)
selected_features=x_train.columns[sfs.get_support()]
print("selescted features:  ",selected_features)

#backward eelemeination
sfs=SequentialFeatureSelector(model,n_features_to_select=5,direction='backward')
sfs.fit(x_train,y_train)
selected_features=x_train.columns[sfs.get_support()]
print("selescted features:  ",selected_features)